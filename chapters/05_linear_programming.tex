\chapter[Linear programming for approximation algorithms]{Linear programming for approximation algorithms}\label{sec:linear-programming}

\textbf{\textsc{Content note}}: \emph{Part of this chapter is taken from \href{https://github.com/Halolegend94/uni_social_behavioral_networks/blob/master/chapters/ch05-densest-subgraph.tex}{this repo} by \href{https://github.com/Halolegend94}{Cristian Di Pietrantonio}.}
\vspace{2ex}

In this chapter Linear Programming is introduced as a tool to build approximation algorithms. In particular, we use the \textit{Densest Subgraph} problem (DSG for short) as a practical example, because some practitioners believe that it is a good primitive to find community of people; also, this problem is in $P$ and we will solve it through linear programming.


\section[Linear Programming]{Linear Programming
\raisebox{.3\baselineskip}{\normalsize\footnotemark}}
\footnotetext{You can read more about this \href{https://github.com/Halolegend94/uni_social_behavioral_networks/blob/master/main.pdf}{here}.}\label{sec:linear-programming-intro}

Linear Programming (\textit{LP}) is arguably the most important technique when it comes to approximation algorithms, and many approximations proved with other methods can be understood as linear programming proof.

\begin{defn}[Primal linear program]\label{primal}
    A linear program is a convex program written in order to solve an optimization problem, whose aim is to maximize an \textbf{objective function} with $n$ variables, of the form
    \begin{equation}
    \max\ c_1 x_1 + c_2 x_2 + \ldots + c_n x_n
    \end{equation}
    with $x_i \in \mathbb{R}_{\geq 0}$, and $m$ constraints of the form
    \begin{equation}
    \begin{cases}
    a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &\leq b_1\\
    a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &\leq b_2\\
    \ \ \ \vdots\\
    a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &\leq b_m\\
    \end{cases}
    \end{equation}
    Since this is a maximization problem, this is conventionally called a \textbf{primal}.
\end{defn}

Note that, if there were no constraints, the optimization problem would have been trivial.

Furthermore, the assumptions that the $x_i$ are non negative is made for simplicity and without loss of generality. In fact, if we had a negative variable, we could obtain it from positive variables, such as $y=x_1-x_2$.

A primal LP can be written in matrix form in the following way:
\begin{equation}
    \begin{aligned}\label{primal-matrix}
        &\max\ \bar{c}^T \bar{x}\\
        &\bar{A} \bar{x} \leq \bar{b}\\
        &\bar{x} \geq \bar{0}
    \end{aligned}
\end{equation}
where $\bar{x} =
\begin{pmatrix}
    x_1 \\ x_2 \\ \ \vdots \\ x_n
\end{pmatrix}, \bar{A} =
\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \ \vdots & \ \vdots &  & \ \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{mn}
\end{pmatrix},\ \bar{b} =
\begin{pmatrix}
    b_1 \\ b_2 \\ \ \vdots \\ b_m
\end{pmatrix},\ \bar{c} =
\begin{pmatrix}
    c_1 \\ c_2 \\ \ \vdots \\ b_n
\end{pmatrix}$.

\begin{defn}(Dual linear program)\label{dual}
    A dual LP can be represented with the following matrix form:
    \begin{equation}
        \begin{aligned}\label{dual-matrix}
            &\min\ \bar{y}^T \bar{b}\\
            &\bar{y}^T\bar{A} \geq \bar{c}\\
            &\bar{y} \geq \bar{0}
        \end{aligned}
    \end{equation}
    Note that it uses the same (transposed) coefficient and constant matrices of the primal, but the variable vector has dimension $m$ instead of $n$. So, to pass from the primal to the dual, we have to change $\max$ into $\min$ and to let any constraint become a variable and vice versa.
\end{defn}

\begin{thm}[Weak Duality]\label{thm:weak-duality}
    If $\bar{x}$ is a feasible solution to the primal and $\bar{y}$ is a feasible solution to the dual, then
    \begin{equation}
        \bar{c}^T \bar{x} \leq \bar{y}^T \bar{b}
    \end{equation}
    I.e., $opt(\text{primal}) \leq  opt(\text{dual})$.
\end{thm}

If we have a primal, then we can guess a solution $\bar{x}$, check that it satisfies the constraints and evaluate how good it is by using the objective function; to know if the solution is optimal or close to optimal, we can write and solve the dual, then we would know the dual's optimum is greater or equal to the primal's. If one guesses a solution for the primal and guesses a solution for the dual, and their costs are close, then that solution is close to the optimal one.

\begin{proof}
    \begin{flalign*}
        &\bar{c}^T \bar{x} \leq (\bar{y}^T \bar{A})\bar{x} = \bar{y}^T (\bar{A}\bar{x}) \leq \bar{y}^T \bar{b}&
    \end{flalign*}
    where the first inequality holds by definition of dual, and the second by definition of primal.
\end{proof}

\begin{thm}[Strong Duality]\label{thm:strong-duality}
    If $\bar{x}$ is an optimal solution to the primal and $\bar{y}$ is an optimal solution to the dual, and if $\bar{c}^T$ and $\bar{y} \bar{b}$ are finite, then
    \begin{equation}
        \bar{c}^T \bar{x} = \bar{y} \bar{b}
    \end{equation}
    I.e., $opt(\text{primal}) = opt(\text{dual})$.
\end{thm}

The strong duality tells us that if both primal and dual admit solution they have the same value for the optimum solution.

\obs It is a common approach to transform a combinatorial problem that one is faced with into a linear one, then solve the linear one (that requires polynomial time), and then transform the solution to the linear problem in a solution to the original problem.


\section{Densest subgraph}\label{sec:densest-subgraph}

\subsection{A linear program to solve the densest subgraph problem} \label{sec:dsp-lp}

We are interested in this problem since it allows to find communities into social networks.

\begin{defn}[Induced subgraph]
    If $G(V,E)$ is a graph and $S \subset G$, then $G[S]$ (read ``$G$ induced on $S$'') is a graph with node set $S$ and edge set $\{ \{u,v\} \st \{u,v\} \in E \wedge u,v \in S \}$.
\end{defn}

\begin{defn}[Densest subgraph]\label{densest-subgraph}
    The densest subgraph of $G(V,E)$ is one induced subgraph $G[S]$ with the largest average degree, i.e., with $S$ such that the density of the graph $f(S) = \frac{|E(S)|}{|S|}$ is maximized.
\end{defn}

\begin{obs}
    Note that, by maximizing the density of the graph, we maximize the average degree of the graph $\frac{2 \abs{E(S)}}{\abs{S}}$.
\end{obs}

\begin{obs}
    This is a particular approximation problem, since it can actually be solved exactly in polynomial time.
\end{obs}

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{densest-subgraph}
    \caption{An example of densest subgraph $G[S]$.}
    \label{fig:densest-subgraph}
\end{figure}

Now we want to express the densest subgraph problem as a (primal) linear problem (we will explain it in detail immediately after the statement):
\begin{equation}
    \begin{aligned}\label{lp-densest-subgraph}
    &\max\sum_{\{i, j\} \in E(G)}X_{\{i, j\}}\\
    &\begin{cases}
        X_{\{i, j\}} \leq Y_i & \forall\ \{i,j\} \in E(G)\\
        X_{\{i, j\}} \leq Y_j & \forall\ \{i,j\} \in E(G)\\
        \sum_{i=1}^n Y_i \leq 1
    \end{cases}\\
    &X_{\{i, j\}}, Y_i \geq 0 \ \ \forall\ \{i,j\} \in E(G)
    \end{aligned}
\end{equation}

Starting from the objective function, we want something that says ``get as many edges as possible in the final graph''; so, it seems natural to maximize the number of edges we can put in the final subset of nodes. Observe, though, that we cannot maximize $f(S)$ since it is not linear. One can take another approach and look only at the numerator. We define a variable $X_{\{i, j\}}$ to be the fractional amount of edge $\{i, j\}$ that is put inside the solution. The objective function is
\begin{equation}
    \max\sum_{\{i, j\} \in E}X_{\{i, j\}}.
\end{equation}

For each edge, we cannot pick that edge fractionally more than the amount with which we pick one of its endpoints. For example, if we pick $\{i, j\}$ for $\frac{1}{2}$ of its entirety, then it is the case that we had to pick both $i$ and $j$ for at least $\frac{1}{2}$. This can be expressed introducing a variable $Y_i$ for each node $i$, representing the fractional amount with which we pick node $i$; then, we can say that
\begin{flalign}\label{eq:x_size_constraint}
    X_{\{i, j\}} &\leq Y_i,\\
    X_{\{i, j\}} &\leq Y_j.
\end{flalign}

Up until now the LP is unbounded. We now need to model the denominator by saying that the total amount with which nodes of the graph are picked must be limited; we can think of that amount being, say, 1 to normalize it. Making the denominator a constant allows us to linearize the objective function.
\begin{equation}\label{eq:dsg_denominator_constraint}
    \sum_{i=1}^n Y_i \leq 1.
\end{equation}

Finally, the variables must be non-negative:
\begin{equation}
    X_{\{i, j\}}, Y_j \geq 0
\end{equation}

What we will be proving is that the optimal solution $OPT_{LP}$ of this LP is equal to the optimal solution of the original densest subgraph problem $f(S^*)$, where $S^*$ is the densest subgraph of $G$. To do so we need the following steps:
\begin{enumerate}
    \item There exists a feasible solution to the LP having value $f(S)$;
    \item $OPT_{LP} \geq f(S^*)$;
    \item $f(S^*) \geq OPT_{LP}$.
\end{enumerate}

\pagebreak

\begin{lem}\label{l:dsp-feasibility}
    For any graph $G(V,E)$ and for any $\emptyset \subset S \subset V$, there exists a feasible solution to the linear problem having value $f(S) = \frac{|E(G[S])|}{|S|}$, which is the subgraph's density.
\end{lem}

\begin{proof}
    We will give a solution and show that it meets the requirements. Define:
    
    \[
        Y_i := \begin{cases}
            \frac{1}{|S|}   & \text{if } i \in S \\
            0               & \text{else}
        \end{cases}
    \]
    
    Remember that the variables $Y_i$ represent how much of each node in the graph we are taking, this definition states that we split the unit among the nodes in the set $S$. Then let:
    
    \[
        X_{\{i, j\}} = \begin{cases}
            \frac{1}{|S|}   & \text{if } \{i, j\} \subseteq S \\
            0               & \text{else}
        \end{cases}
    \]
    Remember that, for simplicity of notation: $\{i, j\} \in E(G[S]) \implies \{i, j\} \subseteq S$ % And the opposite is not true!
    
    For this solution to be feasible, all constraints are checked. Start with Inequality \ref{eq:dsg_denominator_constraint}:
    \[
        \sum_{i} Y_i = |S| \frac{1}{|S|} = 1
    \]
    
    which is satisfied. Then proceed with Inequality \ref{eq:x_size_constraint}, starting with a generic $\{i, j\} \in E$. We have two cases:
    \begin{itemize}
        \item $\{i, j\} \not\subseteq S$: In this case we want to guarantee that
            \begin{equation}
                \begin{cases}
                    X_{\{i, j\}} \leq Y_i\\
                    X_{\{i, j\}} \leq Y_j
                \end{cases}
                \implies X_{\{i, j\}} \leq \min{\{Y_i, Y_j\}},
            \end{equation}
            but both sides of the two equations are zero, because both $i$ and $j$ are not in $S$.
        
        \item $\{i, j\} \subseteq S$: In the same vein, both sides of the two equations evaluate to $\frac{1}{|S|}$.
    \end{itemize}
    
    By that, we proved that the solution is feasible. At this point it's easy to evaluate the objective function:
    \[
        \sum_{\{i, j\} \in E(G)} X_{\{i, j\}} = \abs{E(G[S])} \frac{1}{|S|} = f(S)
    \]
\end{proof}

\begin{cor}
    Let $OPT_{LP}$ be the value of the optimal feasible solution to the LP, and let $S^*$ be the densest subgraph of $G$, then:

    \[
        f(S) = OPT_{LP} \geq OPT = f(S^*) = \frac{\abs{E(S^*)}}{\abs{S^*}}.
    \]
    
    In other words, the consequence of constructing the solution used to prove \ref{l:dsp-feasibility} is that the LP's optimal value is never worse than the original densest subgraph problem's optimum.
\end{cor}

\begin{lem}\label{l:dsp-geq-lp}
    Let $\{X_{\{i, j\}}, Y_i\}$ be an optimal solution to the Linear Program \ref{lp-densest-subgraph}, having value $v$. Then $\exists S \subseteq V : f(S) \geq v$.
\end{lem}

\begin{proof}[Proof by Charikar]
    To prove this lemma, we will proceed step by step claiming some properties that will help us to reach our goal.
    
    \begin{claim}\label{cl:dsp-1}
        For an optimal solution, the following property holds:
        \[
            \forall \{i, j\} \in E(G) \quad X_{\{i, j\}} = \min(Y_i, Y_j)
        \]
    \end{claim}
    \begin{proof}
        Assume that this was not the case: $\exists\ X_{\{i, j\}} < \min{\{Y_i, Y_j\}}$. Then the value of $X_{\{i, j\}}$ can be increased up to $\min(Y_i, Y_j)$, which in turn increases the objective function's value while keepoing the solution feasible. It entails that $X_{\{i, j\}}$ wasn't the optimal value, which is a contradiction.
    \end{proof}

    Now, fix a parameter $r$ and let:
    \begin{align}
        S(r) &:= \{ i \in V(G) : Y_i \geq r\}                   & \label{dsp-sr} \\
        H(r) &:= \{ \{i, j\} \in E(G) : X_{\{i, j\}} \geq r\}   & \label{dsp-er}
    \end{align}

    We are defining a parametric set, that contains all the nodes $i$ such that $Y_i \geq r$ in the optimal solution. $S(0)$ contains all the nodes, and as $r$ increases, it shrinks increasingly. $H(r)$ is the edges' analogue.
    
    \begin{claim}\label{cl:dsp-2}
        No matter which $r$ we pick, it will select a set of nodes such that the selected edges are induced by that set of nodes:
        \[
            \forall\ 0 \leq r \leq \max{\{Y_i\}} \qquad \{i, j\} \in H(r) \iff \{i, j\} \subseteq S(r)
        \]
        Or alternatively:
        \[
            \forall\ 0 \leq r \leq \max{\{Y_i\}} \qquad H(r) = E(G[S(r)])
        \]
    \end{claim}

    \begin{proof}
        The proof is split in the two directions:
        \begin{itemize}
            \item ($\implies$):
                \begin{align*}
                    \{i, j\} \in E(r) & \implies X_{\{i, j\}} \geq r            & \tag{by [\ref{dsp-er}]}           \\
                                      & \implies \min{\{Y_i, Y_j\}} \geq r      & \tag{by claim [\ref{cl:dsp-1}]}   \\
                                      & \implies Y_i \geq r \wedge Y_j \geq r   &                                   \\
                                      & \implies \{i, j\} \subseteq S(r)        & \tag{by [\ref{dsp-sr}]}
                \end{align*}
            \item ($\impliedby$):
                \begin{align*}
                    \{i, j\} \subseteq S(r) & \implies r \leq X_{\{i, j\}} = \min{\{Y_i, Y_j\}} & \tag{by claim [\ref{cl:dsp-1}]}\\
                                            & \implies \{i, j\} \in E(r)                        & \tag{by [\ref{dsp-er}]}
                \end{align*}
        \end{itemize}
    \end{proof}

    Now we introduce an ordering of the variables $Y_i$ and two integrals that will be useful later: let $\pi$ be a permutation over them which preserves the values' natural ordering:
    \[
        0 =: Y_{\pi_{(0)}} \leq Y_{\pi_{(1)}} \leq Y_{\pi_{(2)}} \leq Y_{\pi_{(2)}} \leq \ldots \leq Y_{\pi_{(|V| - 1)}} =: \max{\{Y_i\}}
    \]
    Observe the following integral which gives a property of the nodes:
    
    \begin{figure}[ht]
        \centering
        \includegraphics[scale = 0.4]{dsp-integral}
        \caption{A representation of the integral $\int_0^{\max{\{Y_i\}}} |S(r)| dr$.}
        \label{fig:dsp-integral}
    \end{figure}
    
    \begin{claim}\label{cl:dsp-3}
        \[
            \int_0^{\max{\{Y_i\}}} |S(r)| dr =\sum_{i = 1}^n Y_i.
        \]
    \end{claim}

    \begin{proof}
        First of all, let's note that the integral in [\ref{cl:dsp-3}] can be graphically represented as in the picture [\ref{fig:dsp-integral}], where each gray rectangle is the integral for a certain value of $r$, i.e., an addend of the sum that makes up the integral:
        
        \begin{align*}
            \int_0^{\max{\{Y_i\}}} |S(r)| dr &= \sum_{i = 1}^{n} ( \overbrace{(n - 1 + 1)}^{\text{height}} \cdot \overbrace{(Y_{\pi_{(i)}} - Y_{\pi_{(i-1)}})}^{\text{width}} )& \tag{by geom. interpretation of integral}\\
            &= \sum_{i=1}^{n} \left(\left( n-1+1 \right) \cdot Y_{\pi_{(i)}} \right) - \sum_{i=1}^{n} \left(\left( n-1+1 \right) \cdot Y_{\pi_{(i-1)}} \right)& \\
            &= \sum_{i=1}^{n} \left(\left( n-1+1 \right) \cdot Y_{\pi_{(i)}} \right) - \sum_{i=0}^{n-1} \left(\left( n-1 \right) \cdot Y_{\pi_{(i)}} \right)&\tag{by index shifting}\\
            &= \sum_{i=1}^{n} \left(\left( n-1+1 \right) \cdot Y_{\pi_{(i)}} \right) - \sum_{i=1}^{n} \left(\left( n-1 \right) \cdot Y_{\pi_{(i)}} \right)&\tag{$*^3$}\\
            &= \sum_{i=1}^n Y_{\pi_{(i)}} = \sum_{i=1}^n Y_i&
        \end{align*}
        
        The step marked by $(*^3)$ is due to the fact that for $i=0$ and for $i=n$ the value of the term is $0$, so we can sum from $1$ to $n$ as in the first sum, to obtain compatible addends.
    \end{proof}
    
    \begin{claim}\label{cl:dsp-4}
        \begin{equation}
            \int_0^{\max{\{Y_i\}}} |H(r)|dr = \sum_{i = 1}^n X_{\{i, j\}}.
        \end{equation}
    \end{claim}

    \begin{proof}
        The proof is analogous to the one for the claim \ref{cl:dsp-3}.
    \end{proof}
    
    \begin{claim}\label{cl:dsp-5}
        There exists a value for $r$ in $[0, \max{\{Y_i\}}]$ such that $|H(r)| \geq v|S(r)|$. It entails:
        \[
            f(S(r)) = \frac{|H(r)|}{|S(r)|} \geq v = \sum_{\{i,j\}} X_{\{i, j\}}
        \]
        meaning that, for a specific value for $r$, we have a DSG value that is at least $v$, whhich is our final goal.
    \end{claim}
    
    \begin{proof}
        The proof goes by contradiction. Assume that, $\forall\ r \in [0, \max{\{Y_i\}}]$, it holds that $|E(r)| < v|S(r)|$. Then:
        \[
            \int_0^{\max{\{Y_i\}}}|E(r)| dr < v\int_0^{\max{\{Y_i\}}}|S(r)| dr
        \]
        By claims [\ref{cl:dsp-3}] and [\ref{cl:dsp-4}], it follows that:
        \[
            \sum_{\{i, j\} \in E} X_{\{i, j\}} < v \sum_{i = 1}^n Y_i
        \]
        This contradicts the solution's optimality, which states that:
        \[
            \sum_{i = 1}^n Y_i = 1\ \wedge\ \sum_{\{i, j\} \in E} X_{\{i, j\}} = v
        \]
        and would thus yield $v < v \cdot 1$
    \end{proof}

    At this point we have proved that the LP we gave finds the exact optimal solution for the Densest Subgraph problem.
\end{proof}

As a side note, in this proof we used integrals to avoid to sum over all the possible values of the $Y_i$s. How can we actually find the set $S$ given the LP's optimal value $v$? How many candidate sets there can be? In principle infinitely many, but the only ones that matter are given by $r \in \{Y_i : 1 \leq i \leq n\}$ (because of how $S(r)$ is defined). So we try all the $n$ possible values of $r$ and pick the best one.

Let's note that solving this Linear Problem requires polynomial time, that is pretty good in general, but it is effectively impossible to use for graphs with more than 10000 nodes, such as the social graphs, so we will look for linear and sublinear approximations.


\subsection{A greedy algorithm to solve the densest subgraph problem}\label{sec:dsp-greedy}

Here, we explore a greedy algorithm to approximate the optimal solution in linear time. Recall that $f(S)$ is the function defined in [\ref{densest-subgraph}].

\begin{lstlisting}[caption={The Greedy algorithm to solve the densest subgraph problem},label={lst:dsp-greedy}]
Greedy G(V,E):
    $S_0 \gets V$
    for $i=1, \ldots, n-1$:
        let $v_{i-1} \in S$ be a node of minimum degree in $G[S_{i-1}]$
        $S_i \gets S_{i-1} - \{v_{i-1}\}$
    return the ``best'' $S_i$ (in terms of its value $f(S_i)$)
\end{lstlisting}

\begin{obs}
    This algorithm runs in $O(|V| + |E|)$ time, i.e., in linear time.
\end{obs}

\begin{thm}\label{thm:dsp-greedy}
    The solution returned by Algorithm [\ref{lst:dsp-greedy}] is a 2-approximation of the optimal solution.
\end{thm}

\begin{obs}
    We will see that Greedy can be analyzed as an implicit LP. We introduce here a picture that will be more clear later, but is useful since now to show that the optimal and the greedy solution are sandwiched between the solutions of the dual and the primal problems underlying this algorithm: see figure [\ref{fig:dsp-plot}].
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{dsp-plot}
        \caption{A plot of the optimal and the greedy solution sandwiched between the dual's and the primal's.}
        \label{fig:dsp-plot}
    \end{figure}
\end{obs}

\begin{proof}
    As it often happens in primal-dual proofs, even in this case it will be useful to refer to an underlying algebraic structure, for this proof it will be the \textit{orientation}.
    
    \begin{defn}[Orientation]
        An orientation $\varphi$ of the simple undirected graph $G(V,E)$ is an assignmento f each edge in $E$ to one of its endpoints.
    \end{defn}
    
    \obs There are $2^{\abs{E}}$ possible orientations $\varphi$.
    
    Let's introduce some other definitions that will be useful in the following proof.
    
    \begin{defn}[$\varphi$-degree]
        Given $\varphi$ and $v \in V$, let $d_\varphi(v) = \abs{\{ e \st v \in e,\ e \text{ oriented towards } v \text{ by } \varphi \}}$.
    \end{defn}
    
    \begin{defn}[Maximum $\varphi$-degree]
        Given $\varphi$ and $v \in V$, let $\Delta_\varphi = \max_{v \in V}\{d_\varphi(v)\}$.
    \end{defn}

    \pagebreak
    
    What we want to do now is to bound the quality of the optimal solution in terms of some property of all orientations of the graph $G$. The reason is that the algorithm will be analyzed by making it implicitly select an orientation during its execution. Since we are about to show we can bound the value of the optimal solution to DSG by some function of any orientation, this will allow us to give a bound on the quality of the algorithm.
    
    \begin{lem}\label{l:dsp-greedy-1}
        \begin{equation}
            \forall\ \varphi \max_{\emptyset \subset S \subseteq V}\{f(S)\} \leq \Delta_\varphi.
        \end{equation}
    \end{lem}
    
    \begin{proof}
        \begin{flalign*}
            |E(S)| &\leq \sum_{v \in S} d_\varphi(v)&\tag{$*$}\\
            &\leq \sum_{v \in S} \Delta_\varphi(v)&\tag{we upperbound each $d_\varphi(v)$ with $\Delta_\varphi(v)$}\\
            &= |S| \cdot \Delta_\varphi(v)&\\
            &\Downarrow&\tag{by dividing by $|S|$}\\
            \frac{|E(S)|}{|S|}&\leq \Delta_\varphi&
        \end{flalign*}
        The reason of the step marked by $*$ is the following: Pick $e = \{v, w\} \in E(S)$, the orientation $\varphi$ will orient the edge $e$ towards $v$ or $w$, that is, towards some node of $S$, and therefore it will be counted in the sum; Moreover, if we sum up the $\varphi$-degrees, we will be also counting edges that come from nodes outside $S$ to nodes in $S$.
        
        The fact that $f(S) = \frac{|E(S)|}{|S|}$ concludes the proof.
    \end{proof}

    This prove the upper bound part of Theorem [\ref{thm:dsp-greedy}], that is valid $\forall\ \varphi$; now we want to find a lower bound that is close to the optimal solution of the dual LP underlying Greedy [\ref{lst:dsp-greedy}] ($b$ in figure [\ref{fig:dsp-plot}]), and that holds for Greedy as well as for the optimal solution given by the LP [\ref{lp-densest-subgraph}].
    
    This bound will be valid only for the $\varphi$ implicitly built by Greedy, that we will call $\varphi_G$, defined as follows:
    \begin{itemize}
        \item The orientation is created as the algorithm progresses;
        \item At the beginning no edge is directed towards anything;
        \item When Greedy remove a node $w_i$, all the edges incident in $w_i$ and that are still in the graph will be oriented towards $w_i$.
    \end{itemize}
    An example of $\varphi_G$ is given in figure [\ref{fig:dsp-phi}].
    
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{dsp-phi}
        \caption{Example of computation of $\varphi_G$}
        \label{fig:dsp-phi}
    \end{figure}

    Note that the algorithm doesn't care about orientations, but we will use this concept in the proof.
    
    \begin{lem}\label{l:dsp-greedy-2}
        Let $M$ be the solution returned by Greedy, i.e., $M := \max_{i = 0, \ldots, n-1}\{f(S_i)\}$, then the following inequality holds:
        \begin{equation}
            \Delta_{\varphi_G} \leq 2M
        \end{equation}
    \end{lem}
    \begin{proof}
        Pick any $S_i$, let $v_i$ be a node of minimum degree in $G[S_i]$:
        \begin{flalign*}
            d_\varphi(v_i) &= \deg_{S_i}(v_i)&\tag{if I remove $v_i$, then $d\varphi(v_i) = d_{S_i}(v_i)$}\\
            &= \min_{v \in S_i}\deg_{S_i}(v)&\\
            &\leq \avg_{v \in S_i}\deg_{S_i}(v)&\\
            &=\frac{1}{|S_i|}\sum_{v \in S_i}\deg_{S_i}(v)&\\
            &=2\frac{|E(S_i)|}{|S_i|} = 2f(S_i) \leq 2M.&
        \end{flalign*}
    \end{proof}

    \begin{lem}\label{l:dsp-greedy-3}
        \begin{equation}
            \max_{\emptyset \subset S \subseteq V} f(S) \leq 2M
        \end{equation}
    \end{lem}
    \begin{proof}
        Apply lemma [\ref{l:dsp-greedy-1}] with $\varphi = \varphi_G$ together with lemma [\ref{l:dsp-greedy-2}].
    \end{proof}

    This finally concludes the proof of Theorem [\ref{thm:dsp-greedy}].
\end{proof}

\obs To obtain the actual approximated densest subgraph from Greedy, it isn't necessary to store each one of the $S_i$s, it is sufficient to keep in memory the degree of the node $v_i$ we deleted. Furthermore, by returning the obtained community and $\deg(v_i)$, we have a proof of the goodness of that community, since the degree gives a bound for the optimal community.

Now we are going to show the \textit{implicit linear problem} underlying Greedy.\\
The \textbf{primal} LP is the same we saw in the previous section, i.e. [\ref{lp-densest-subgraph}], we just rename some variables and give a name to the constraints (in square brackets):
\begin{equation}\label{lp-greedy-primal}
    \begin{aligned}
        &\max\sum_{\{i, j\} \in E(G)}X_{\{i, j\}}&\\
        &\begin{cases}
            X_{\{i, j\}} - X_i \leq 0 & \forall\ \{i,j\} \in E(G) \quad\quad [Y_{i,j}]\\
            X_{\{i, j\}} - X_j \leq 0 & \forall\ \{i,j\} \in E(G) \quad\quad [Y_{j,i}]\\
            \sum_{i=1}^n X_i \leq 1 & \phantom{\forall\ \{i,j\} \in E(G)} \quad\quad [Y^*]
        \end{cases}&\\
        &X_{\{i, j\}}, X_i \geq 0 \ \ \forall\ \{i,j\} \in E(G)&
    \end{aligned}
\end{equation}
The \textbf{dual} LP is the following:
\begin{equation}\label{lp-greedy-dual}
    \begin{aligned}
        &\min Y^*&\\
        &\begin{cases}
            Y_{i, j} +  Y_{j,i} \geq 1 & \forall\ \{i,j\} \in E(G) \quad\quad [X_{\{i,j\}}]\\
            Y^* - \sum_{j \forall \{i,j\} \in E(G)} Y_{i, j} \geq 0 & \forall\ i \in V(G) \phantom{\{,j\}} \quad\quad [X_i]
        \end{cases}&\\
        &Y_i, Y_j, Y^* \geq 0&
    \end{aligned}
\end{equation}

Now we give a feasible solution for the primal and a feasible solution for the dual.\\
Suppose that Greedy outputs the set $S$, the \textbf{primal solution} is the following:
\begin{equation}
    \begin{aligned}
    &X_i =
    \begin{cases}
        \frac{1}{|S|} & \text{if } i \in S\\
        0             & \text{otherwise}
    \end{cases}&\\
    &X_{i,j} =
    \begin{cases}
        \frac{1}{|S|} & \text{if } i,j \in S\\
        0             & \text{otherwise}
    \end{cases}&
    \end{aligned}
\end{equation}
\begin{proof}[Proof of feasibility]
    Given in the previous section, see the proof of Lemma [\ref{l:dsp-feasibility}].
\end{proof}
Suppose that Greedy produces $\varphi_G$, the \textbf{dual solution} is the following:
\begin{equation}
    \begin{aligned}
        &Y_{i,j} =
        \begin{cases}
            1 & \text{if } \{i,j\} \in E(G) \text{ and is directed towards } i \text{ according to } \varphi_G\\
            0 & \text{otherwise}
        \end{cases}&\\
        &Y_{j,i} = 1 - Y_{i,j}&\\
        &Y^* = \Delta_{\varphi_G}&
    \end{aligned}
\end{equation}
\begin{proof}[Proof of feasibility]
    The first constraint is satisfied since each edge $\{i,j\}$ is always oriented by $\varphi_G$ either towards $i$ or towards $j$, so $Y_{i,j} + Y_{j,i} = 1$.\\
    The second constraint is satisfied because $Y^* - \sum_{j \forall \{i,j\} \in E(G)} Y_{i, j} = Y^* - d_{\varphi_G} \geq 0$, since the optimal solution of the dual is $\Delta_{\varphi_G} = Y^* \geq d_{\varphi_G}$.
\end{proof}

By this we shown that the analysis of Greedy can be seen as a primal-dual proof, i.e., the solution of Greedy is sandwiched between the primal solution and the dual solution (in this particular case, the solution of Greedy is exactly the same as the solution of primal). Furthermore, this concludes the explanation of the figure [\ref{fig:dsp-plot}].

\begin{ex}
    Now we are going to apply the primal-dual approach to a simple yet useful example of LP underlying Greedy.
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{dsp-ex}
        \caption{Example of primal-dual approach to Densest Subgraph problem.}
        \label{fig:dsp-ex}
    \end{figure}

    First of all, we apply the primal LP [\ref{lp-greedy-primal}] to the graph in the picture [\ref{fig:dsp-ex}] and we obtain
    \begin{align*}
        &\max X_{\{1,2\}} + X_{\{1,2\}} \left( + 0X_1 + 0X_2 + 0X_3 \right)\\
        &\begin{cases}
            X_{\{1,2\}} - X_1 & \leq 0 \\
            X_{\{1,2\}} - X_2 & \leq 0 \\
            X_{\{2,3\}} - X_2 & \leq 0 \\
            X_{\{2,3\}} - X_3 & \leq 0 \\
            X_1 + X_2 + X_3   & \leq 1
        \end{cases}\\
        &X_{\{i, j\}}, X_i \geq 0 \ \ \forall\ \{i,j\} \in E(G)
    \end{align*}
    From this, we obtain the matrix form described in [\ref{primal-matrix}], that we represent here, together with the dual matrix presented in [\ref{dual-matrix}]:\\
    \begin{minipage}{0.4\textwidth}
        \begin{flalign*}
            &\text{Primal:}\\
            &\max\ \bar{c}^T \bar{x}\\
            &\bar{A} \bar{x} \leq \bar{b}\\
            &\bar{x} \geq \bar{0}
        \end{flalign*}
    \end{minipage}
    \begin{minipage}{0.2\textwidth}
        \begin{flalign*}
            &\\
            &\\
            &\Longrightarrow\\
            &
        \end{flalign*}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \begin{flalign*}
            &\text{Dual:}\\
            &\min\ \bar{y}^T \bar{b}\\
            &\bar{y}^T\bar{A} \geq \bar{c}\\
            &\bar{y} \geq \bar{0}
        \end{flalign*}
    \end{minipage}

    In our example we have: $\bar{x} =
    \begin{pmatrix}
        X_{\{1,2\}} \\ X_{\{2,3\}} \\ X_1 \\ X_2 \\ X_3
    \end{pmatrix}, \bar{A} =
    \begin{pmatrix}
    1 & 0 & -1 & 0 & 0 \\
    1 & 0 & 0 & -1 & 0 \\
    0 & 1 & 0 & -1 & 0 \\
    0 & 1 & 0 & 0 & -1 \\
    0 & 0 & 1 & 1 & 1 \\
    \end{pmatrix},\ \bar{b} =
    \begin{pmatrix}
        0 \\ 0 \\ 0 \\ 0 \\ 1
    \end{pmatrix},\ \bar{c} =
    \begin{pmatrix}
        1 \\ 1 \\ 0 \\ 0 \\ 0
    \end{pmatrix}$.\\
    Note that matrix $\bar{A}$ has one row for each constraint and one column for each variable.
    
    Now we can compute the dual of our example: for the moment we take a vector $\bar{y}$ with one variable for each constraint in the primal
    $\bar{y}^T = \begin{pmatrix} Y_1 & Y_2 & Y_3 & Y_4 & Y_5 \end{pmatrix}$, later we will assign more significant names to the variables.
    \begin{flalign*}
        \min \ \bar{y}^T \bar{b} &\Rightarrow min \begin{pmatrix} Y_1 & Y_2 & Y_3 & Y_4 & Y_5 \end{pmatrix} \cdot \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} = Y_5\\
        \bar{y}^T\bar{A} \geq \bar{c} &\Rightarrow \begin{pmatrix} Y_1 + Y_2 \\ Y_3 + Y_4 \\ -Y_1 + Y_5 \\ -Y_2-Y_3+Y_5 \\ -Y_4+Y_5 \end{pmatrix} \geq \begin{pmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{pmatrix}
    \end{flalign*}
    
    From this formulation we can obtain our constraints:
    \begin{flalign*}
        \begin{cases}
            Y_1 + Y_2 &\geq 1\\
            Y_3 + Y_4 &\geq 1\\
            -Y_1 + Y_5 &\geq 0\\
            -Y_2-Y_3+Y_5 &\geq 0\\
            -Y_4+Y_5 &\geq 0
        \end{cases}
    \end{flalign*}
    
    Finally, we can rename the $Y_i$ variables to adapt our result to the LP in [\ref{lp-greedy-dual}]: $Y_1 \rightarrow Y_{1,2}, \ Y_2 \rightarrow Y_{2,1}, \ Y_3 \rightarrow Y_{2,3}, \ Y_4 \rightarrow Y_{3,2}, \ Y_5 \rightarrow Y^*$; that gives us the following LP:
    \begin{equation*}
    \begin{aligned}
    &\min Y^*\\
    &\begin{cases}
    Y_{1,2} + Y_{2,1} &\geq 1\\
    Y_{2,3} + Y_{3,2} &\geq 1\\
    Y^* - Y_{1,2} &\geq 0\\
    Y^* - Y_{2,1} - Y_{2,3} &\geq 0\\
    Y^* - Y_{3,2} &\geq 0\\
    \end{cases}
    \end{aligned}
    \end{equation*}
    
    This allows us to see again the connection between those variables and constraint and the original graph.
\end{ex}


\subsection{A sublinear algorithm to solve the densest subgraph problem} \label{sec:dsp-sublinear}

We now have a linear algorithm to approximate the DSG solution. But being linear in a graph with billion of nodes is still unfeasible, though. People have tried to improve this algorithm under the assumption that there is a cluster of computers each of which is computing towards finding the best solution.

\begin{obs}
    In Greedy algorithm [\ref{lst:dsp-greedy}] we don't really need to remove the node with minimum degree, since the only step in which we used the minimum degree value is in the proof of Lemma [\ref{l:dsp-greedy-2}], but right after we upper bounded it with the average degree. In fact the algorithm could have picked a node with degree less or equal to the average degree and the proof still would have worked.
\end{obs}

Before proceeding with the updated algorithm, let's give some counterexamples that show why it is not sufficient to pick and remove nodes with degree $\leq k \cdot \min_{v \in S_i} \deg_{S_i}(v)$.

\begin{ex}
    If we pick at each iteration all the nodes with at most twice the minimum degree, there is a chance that we remove only two nodes per round, for example if our graph is a path.
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{path}
        \caption{Example of path.}
        \label{fig:dsp-path}
    \end{figure}
\end{ex}


\begin{ex}
    If we pick at each iteration all the nodes with at most $k$ times the minimum degree, with any integer $k \geq 2$, we could remove many nodes at each step, but always in constant number, if we have a graph $G(V,E)$ such that:
    \begin{itemize}
        \item The graph has $n$ nodes and $\sqrt{n}$ layers $L_i$,
        \item Each layer $L_i$ contains $i$ nodes,
        \item $V = \bigcup_{i=1}^k L_i$,
        \item $E = \{ \{v,w\} \st \exists\ v \in L_i \wedge w \in L_{i+1} \}$.
    \end{itemize}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{dsp-layers}
        \caption{Example of graph with more layers.}
        \label{fig:dsp-layers}
    \end{figure}
\end{ex}

\begin{obs}
    The algorithm we are looking for must have two properties:
    \begin{enumerate}
        \item It requires logarithmic number of operations,
        \item Its result is an approximation of the optimal solution by a constant factor.
    \end{enumerate}
\end{obs}

Finally we present the algorithm $DS_\varepsilon$ by Bahmani, Kumar, Vassilvitskii:
\begin{lstlisting}[caption={The $DS_\varepsilon$ algorithm to solve the densest subgraph problem},label={lst:dsp-dse}]
$DS_\varepsilon(G(V,E))$:
    $i \gets 0$
    $S_0 \gets V$
    while $S_i \neq \emptyset$:
        $A_i \gets \{ v \st v \in S_i \wedge \deg_{S_i}(v) \leq 2 \cdot (1+\varepsilon) \cdot f(S_i) = (1+\varepsilon) \cdot \avg_{w \in S_i} \deg_{S_i}(w) \}$
        $S_{i+1} \gets S_{i} - A_i$
    return $\argmax_{S_i} f(S_i)$
\end{lstlisting}

\obs $DS_\varepsilon$ is a parallelization of Greedy in the sense that it removes many nodes all together, i.e., an number of nodes that increases in each iteration.

\begin{lem}\label{l:dse-approx}
    $DS_\varepsilon$ returns a $(2+2\varepsilon)$-approximation.
\end{lem}
\begin{proof}
    Let $S^*$ be an optimal solution to the densest subgraph problem on $G(V,E)$.
    
    \begin{claim}\label{cl:dse-1}
   		\begin{equation}
            \forall v \in S^* \deg_{S^*}(v) \geq f(S^*).
        \end{equation}
    \end{claim}
    It is a pretty intuitive claim: If we have a node in the optimal solution that has a degree less than the average degree, we can remove that node to increase the quality of the solution.
    \begin{proof}
        \begin{flalign*}
            \frac{\abs{E(S^*)}}{|S^*|} = f(S^*) &\geq f(S^* - \{v\}) = \frac{\abs{E(S^* - \{v\})}}{|S^*| - \{v\}} = \frac{\abs{E(S^*)} - \deg_{S^*}(v)}{|S^*|-1}&\tag{by optimality}\\
            &\Downarrow&\\
            \frac{\abs{E(S^*)}}{|S^*|} \cdot (|S^*| -1) &\geq \abs{E(S^*)} - \deg_{S^*}(v)&\\
            &\Downarrow&\\
            \abs{E(S^*)} - \frac{\abs{E(S^*)}}{|S^*|} &\geq \abs{E(S^*)} - \deg_{S^*}(v)&\\
            \deg_{S^*}(v) &\geq \frac{\abs{E(S^*)}}{|S^*|} = f(S^*)&
        \end{flalign*}
    \end{proof}

    Now let's assure that the algorithm terminates. Observe that $A(S_i)$ is always not empty because there will be always some node with degree less than or equal to the average degree. It follows that at every iteration we remove at least one node. We formalize it in the following claim:
    \begin{claim}\label{cl:dse-2}
        \begin{equation}
            |A_i| \geq 1,\ if |S_i| \geq 1.
        \end{equation}
    \end{claim}
    \begin{proof}
        \begin{flalign*}
            \sum_{v \in S_i} \deg_{S_i}(v) &= f \cdot f(S_i) \cdot |S_i|&\\
            \min_{v \in S_i} \deg_{S_i}(v) \leq \avg_{v \in S_i} \deg_{S_i}(v) &= 2 \cdot f(S_i)
        \end{flalign*}
    \end{proof}

    Now we can go back to the proof of Lemma [\ref{l:dse-approx}]. So, at this point, we want to show that there exists one iteration where the quality of the solution in that iteration is a good approximation to the optimal quality.
    
    Fix an optimal solution $S^*$. Let $i$ be the first iteration such that $DS_\varepsilon$ removes some element of $S^*$ from the graph. Notice that there must be such $i$. Formally,
    \begin{equation*}
    A_i \cap S^* \neq \emptyset\ \wedge\ \forall\ j < i\ Aj \cap S^* = \emptyset.
    \end{equation*}
    Let $v \in A_i \cap S^*$. Observe that $S^* \subseteq S_i$, since $S_i$ is just $S^*$ where we removed at least one node. Then the following holds:
    \begin{flalign*}
        f(S^*) &\leq \deg_{S^*}(v)&\tag{by Claim [\ref{cl:dse-1}]}\\
        &\leq \deg_{S_{i}}(v)&\tag{since $S^* \subseteq S_i$}\\
        &\leq 2 \cdot(1 + \varepsilon) \cdot f(S_{i})&\tag{by greedy choice made by [\ref{lst:dsp-dse}]}\\
        &\Downarrow&\\
        f(S_i)&\geq \frac{1}{2(1+\varepsilon)}&
    \end{flalign*}
    And with this we have proven the desired approximation.
\end{proof}

\begin{lem}\label{l:dse-time}
    Let $n=|V|$, $DS_\varepsilon$ terminates after $O\left( \frac{\log n}{\varepsilon}\right)$.
\end{lem}
\begin{proof}
    Fix an iteration $i$, the total degree is:
    \begin{flalign*}
        2|E(S_i)| &= \sum_{v \in S_i} \deg_{S_i}(v)&\\
        &= \sum_{v \in A_i} \deg_{S_i}(v) + \sum_{v \in S_i - A_i} \deg_{S_i}(v)&\\
        &\geq \sum_{v \in A_i} 0 + \sum_{v \in S_i - A_i} (2 \cdot (1+\varepsilon) \cdot f(S_i))&\tag{by greedy choice made by [\ref{lst:dsp-dse}]}\\
        &= |S_i - A_i| \cdot 2 \cdot (1 + \varepsilon) \cdot f(S_i)&\\
        &= \left(|S_i| - |A_i|\right) \cdot 2 \cdot (1 + \varepsilon) \cdot f(S_i)&\tag{because $A_i \subseteq S_i$}\\
        &= \left(|S_i| - |A_i|\right) \cdot 2 \cdot (1 + \varepsilon) \cdot \frac{|E(S_i)|}{|S_i|}&
    \end{flalign*}
    It follows that:
    \begin{flalign*}
        &2|E(S_i)| \geq \left(|S_i| - |A_i|\right) \cdot 2 \cdot (1 + \varepsilon) \frac{|E(S_i)|}{|S_i|}&\\
        &\implies 1 \geq \left(|S_i| - |A_i|\right) \cdot (1+\varepsilon) \frac{1}{|S_i|} = \left(1 - \frac{|A_i|}{|S_i|}\right) \cdot (1+\varepsilon) = 1 + \varepsilon - (1+\varepsilon) \cdot \frac{|A_i|}{|S_i|}&\\
        &\implies (1+\varepsilon) \cdot \frac{|A_i|}{|S_i|} \geq \varepsilon&\\
        &\implies |A_i| = |S_i| - |S_i + 1| \geq \frac{\varepsilon}{1+\varepsilon} \cdot |S_i|&\\
        &\implies |S_i + 1| \leq \left(1 - \frac{\varepsilon}{1+\varepsilon}\right) \cdot |S_i| = \frac{1}{1+\varepsilon} \cdot |S_i|&
    \end{flalign*}
    
    We have proved that $|S_i|$ decreases exponentially. Let's see why in detail:
    \begin{itemize}
        \item $|S_0| = n$,
        \item $|S_1| \leq \left(1 - \frac{\varepsilon}{1+\varepsilon}\right) \cdot |S_0| = \left(1 - \frac{\varepsilon}{1+\varepsilon}\right) \cdot n$,
        \item $|S_2| \leq \left(1 - \frac{\varepsilon}{1+\varepsilon}\right) \cdot |S_1| = \left(1 - \frac{\varepsilon}{1+\varepsilon}\right)^2 \cdot n$,
        \item by induction, $|S_i| \leq \left(1 - \frac{\varepsilon}{1+\varepsilon}\right)^i \cdot n$,
        \item let $I = \lceil \log_{i+\varepsilon} n \rceil$, then $\left(1 - \frac{\varepsilon}{1+\varepsilon}\right)^I = \left( \frac{1}{1 + \varepsilon} \right)^I = (1+\varepsilon)^{-I} \leq (1+\varepsilon)^{-log_{1+\varepsilon n}} = \frac{1}{n}$,
        \item thus, $|S_I| \leq \left(1 - \frac{\varepsilon}{1+\varepsilon}\right)^I \cdot n < 1$.
    \end{itemize}
    That means that the algorithm [\ref{lst:dsp-dse}] will terminate after $I$ iterations, and so it is sublinear, as we wanted to proof.
\end{proof}

To conclude, we can say that not only this algorithm is very efficient, but it can also be easily implemented in parallel with frameworks such as MapReduce or Pregel.
