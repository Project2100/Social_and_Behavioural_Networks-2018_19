%!TEX root=../main.tex
\chapter{Communication Complexity}

In a distributed computing framework like MapReduce, not only it is to have computationally efficient algorithms, but also to avoid overloading the communication channels with too much information. This kind of problem is often referred as \emph{Communication Complexity}.

\section{String equality}
The protocol in figure \ref{lst:fS-greedycc} decides if two given binary strings are equal. One side $A$ samples some bits from its own string, \textsc{xor}s them, and sends the result to the partner $B$. $B$ performs the same task as $A$ picking the bits in the same positions as $A$'s, and compares the two results.
\begin{lstlisting}[caption={Greedy algorithm},label={lst:fS-greedycc}]
$\overline{X}$: bits string of computer $A$
$\overline{Y}$: bits string of computer $B$
Algo(X,Y):
    let $S \subseteq [n]$ be the set of indexes sampled u.a.r.
    let computer $A$ compute $a = \bigoplus_{i \in S} x_{i}$
    $A$ sends $a$ to $B$
    $B$ computes $b = \bigoplus_{i \in S} y_{i}$
    return $[a = b]$
\end{lstlisting}

If $a$ and $b$ are the same string, then the algorithm will always decide $\top$; conversely, a $\top$ result does not necessarily imply that $a$ is equal to $b$. If the algorithm returns $\bot$, then $a \neq b$ with certainty. As we will see, when $a$ and $b$ are different, the algorithm will return $\bot$ with probability $\half$;
We can thus perform $k$ runs of the algorithm. If it never returns $DIFF$ we can say that $a = b$ with confidence $1 - \left( \half \right)^k$. The first time the algorithm returns $\bot$ we can stop because $a \neq b$.\footnote{This principle is a fundamental of \emph{model checking}}

\begin{theorem}
    If $a \neq b$ the algorithm says $\bot$ with probability $\half$
\end{theorem}

\begin{proof}
    Let $T$ be the set of indices in which $\overline{X}$ and $\overline{Y}$ differ:
    \[
        T = \{ i \in [n]: x_i \neq y_i\} 
    \]
    Among them, collect those indices that are sampled by the algorithm in the set $D$:
    \[
        D = S \cap T 
    \]
    $D$ distributes \uar{} because is analogous to $S$, which in turn is \uar{} by definition. Recall that $A$ will compute $\bigoplus_{i \in S} x_i$ separately from $B$, who in turn will compute $\bigoplus_{i \in S} y_i$, and that \textsc{xor} is a commutative operator:
    \begin{align*}
        a &= \bigoplus_{i \in S} x_i = \left( \bigoplus_{i \in D} x_i \right) \oplus \left( \bigoplus_{i \in S \setminus D} x_i \right) \\
        b &= \bigoplus_{i \in S} y_i = \left( \bigoplus_{i \in D} y_i \right) \oplus \left( \bigoplus_{i \in S \setminus D} y_i \right)
    \end{align*}

    Observe that the \textsc{xor} of the bits at the indices in $S \setminus D$ is the same for $A$ and for $B$. Let $z$ represent this quantity:
    \begin{align*}        
        z = \bigoplus_{i \in S \setminus D} x_i &= \bigoplus_{i \in S \setminus D} y_i
    \end{align*}

    Since $z$ is common to $a$ and $b$, then they are different iff $\bigoplus_{i \in D} x_i$ is different from $\bigoplus_{i \in D} y_i$:
    \begin{align}
        \bigoplus_{i \in S} x_i \neq \bigoplus_{i \in S} y_i \iff \bigoplus_{i \in D} x_i \neq \bigoplus_{i \in D} y_i 
    \end{align}
    By definition of \textsc{xor}:
    \begin{align}
        \bigoplus_{i \in D} x_i \neq \bigoplus_{i \in D} y_i \iff \left(\bigoplus_{i \in D} x_i\right) \oplus \left(\bigoplus_{i \in D} y_i\right) = 1
    \end{align}
    Furthermore, by distributive property:
    \begin{align}
        \left(\bigoplus_{i \in D} x_i\right) \oplus \left(\bigoplus_{i \in D} y_i\right) = \bigoplus_{i \in D}\left( x_i \oplus y_i \right)
    \end{align}
    Since $x_i \oplus y_i = 1 \quad \forall i \in D$ by definition of $D$.
    \begin{align}
        \bigoplus_{i \in D}( x_i \oplus y_i ) = \bigoplus_{i \in D} 1
    \end{align}
    \textsc{xor}ing a certain number of $1$-bits is equivalent to deciding whether the number of \textsc{xor}ed bits is even or not:
    \begin{align}
        \bigoplus_{i \in D} 1 = 
        \begin{cases}
            0 & \textsc{if } \abs{D} \equiv_2 0 \\
            1 & \textsc{else}
        \end{cases}
    \end{align}
    Putting all the pieces together:
    \todo{verify}
    \begin{align}
        \Pr \left[ \bigoplus_{i \in S} x_i \neq \bigoplus_{i \in S} y_i \right] = \Pr[\abs{D} \equiv_2 0] = 
        \begin{cases}
            0 & \textsc{if } \abs{T} = 0 \\
            \half & \textsc{else}
        \end{cases}
    \end{align}
\end{proof}

We have shown what happens when $a \neq b$, now we are going to show that the algorithm will never make a mistake when the two strings are effectively equal.

\begin{claim}
    If $a = b$ the algorithm will always return $\top$.
\end{claim}
\begin{proof}
    If $a = b$ then $|T| = 0$; therefore:
    \[
        \Pr \left[ \bigoplus_{i \in S} x_i = \bigoplus_{i \in S} y_i \right] = 1 - \Pr[\abs{D} \equiv_2 0] = 1 - 0 = 1
    \] 
\end{proof}
    

\section{Connected components}

Let $G = (V, E)$ be a graph, and partition $V$ into $k$ blocks $V_i$. We will mostly be interested in the case $k = n$.

There are $k$ computers with unbounded memory and computational power. Each computer $i$ knows the vertices $v$ in $V_i$, along with all the neighbouring ones $N(v)$. More precisely, computer $i$ has the following set as input:
\[
    \{(v, N(v)) : v \in V_i\}
\]

Computers can interact with each other \emph{point-to-point}: a computer $i \in [k]$ can send any string of bits $\overline{w} \in \binary^*$ to the generic computer $j \in [k] - \{i\}$; the cost of sending a string is equal to its number of bits. The goal of the $k + 1$ computers is to find the connected components of $G$, while minimizing the number of bits sent through the communication channels. 

\subsection{Exact algorithms}

Let $m = \abs{E(G)}$. First, observe that there exists the trivial protocol of sending all necessary info to one single computer, and let it do all the work; the computation is said to be \emph{centralized}.
\begin{theorem}
    The centralized protocol uses $O(m \log n)$ bits of communication.
\end{theorem}

\begin{proof}
    Let $i$ be the computer responsible for the actual computations. All other computers $j \neq i$ will send their own input to computer $i$, in order for the latter to proceed. The total amount of information sent throughout the network is essentially the whole graph, which has complexity $O(m \log n)$.
\end{proof}

The above protocol is indeed forsaking the computational power available on multiple devices, relying on just one of them. The next protocol actually leverages this power in order to achieve the same goal with a different time complexity bound.

Initialize a label for all graph vertices with a unique value, then designate a computer $c$ to be the \emph{coordinator}. Loop over the steps:
\begin{enumerate}
    \item each computer sends a pair of adjacent nodes in its own limited view of the graph, if any, to the coordinator;
    \item the coordinator, upon receiving all couples, picks one \uar{}, and then instructs all the other computers to update the vertex's label with higher value to the other vertex's label;
    \item the other computers perform the update as instructed by the coordinator.
\end{enumerate}
The algorithm ends if no distinct adjacent labels are found in the loop's first step. On termination, the coordinator can easily compute the number of components: it will be equal to the number of distinct labels appearing in the graph, or equivalently it will be equal to $n$ minus the number of completed iterations.

\begin{theorem}
    The protocol uses $O((k - 1) n \log n)$ bits of communication.
\end{theorem}
\begin{proof}
    Data transmission happens only during the first two steps of each loop iteration, each one consisting of $O((k - 1) \log n)$ bits of transmitted data. The number of transmitted bits is then equal to $O((k-1) \log n)$ times the number of completed iterations; also, in each completed iteration, one label is removed. Since the graph starts with $n$ distinct labels, the total cost is $O(n (k-1) \log n)$ bits.
\end{proof}


\subsection{A heuristic approach}

%The algorithm that will be presented has a communication cost of $O\left(n^{3/2} \log^{1/2} n\right)$-bits.

%\cite{cks15}
%\cite{newman91}
The protocol that follows can be made to work in the \emph{simultaneous message} model: the communication can end after a single round. For simplicity of exposition, assume that the computers preemptively share a hash function $h_S: E(G) \to \binary$, with $S$ picked \uar{},\footnote{A result of Newman can be used to translate any simultaneous message algorithm into a $O(1)$-rounds protocol, using private randomness and without any significant additional bits of communication. Thus, the assumption that the computers share a common random source can be avoided.} and that each computer holds exactly one node, so that $k = n$. Also, designate again one computer $c$ as the coordinator.

\subsubsection{Randomized cut checking algorithm}

The protocol makes use of graph cuts, this motivates the design of this subprotocol, which can probabilistically check whether vertex subset induces cuts on some edges of the original graph.

\begin{lstlisting}[caption = {The cut-checking protocol.}, label = alg:cutcheck]
$\forall v \in V$
    $\eta(v) \gets \bigoplus_{u \in N(v)} h_S(\{u, v\})$
    send($c$, $x(v)$)
\end{lstlisting}

\begin{lemma}
    The cut-checking protocol uses $O(n)$ bits of communication.
\end{lemma}

\begin{proof}
    Each computer/vertex sends just one bit of information to the coordinator, thus the amount of data traversing the network is of $n - 1$ bits.
\end{proof}

\begin{lemma}
    Given an arbitrary vertex subset $SV$, let $y_S = \bigoplus_{v \in S} \eta(v)$; then:
    \begin{itemize}
        \item if there are no edges crossing $S$, then $y_S = 0$ with probability $1$;
        \item if there is at least one edge that crosses $S$, then $y_S = 1$ with probability $1/2$.
    \end{itemize}
\end{lemma}

\begin{proof}
    Let $I(S)$ be the set of edges with both endpoints in $S$, and $C(S)$ be the set of edges that have exactly one endpoint in $S$. Then:
    \todo{Elaborate on change from S to I and C}
    \begin{align*}
            y_S
        =&\ \bigoplus_{v \in S} \eta(v)                                                         & \\
        =&\ \bigoplus_{v \in S} \bigoplus_{u \in N(v)} h_S(\{u, v\})                            & \\
        =&\ \bigoplus_{e \in I(S)} (h_S(e) \oplus h_S(e)) \oplus \bigoplus_{e \in C(S)} h_S(e)  & \\
        =&\ \bigoplus_{e \in C(S)} h_S(e)                                                       & (\forall x \; x \oplus x = 0)
    \end{align*}

    If $C(S)$ is empty, then no edges cross $S$, thus $y_S$ will always evaluate to $0$. If instead there are edges in $C(S)$, choose arbitrarily an edge $f \in C(S)$. Then:
    \[
        y_S = h_S(f) \oplus \bigoplus_{e \in C(S) \setminus \{f\}} h_S(e)
    \]
    Since the hashes of $h_S$ are \iid{}, then regardless of all other edges, the output of $h_S(f)$ will guarantee that $y_S$ will distribute uniformly at random in $\binary$; the probability of $y_S$ to be $1$ is thus $\half$.
\end{proof}

\subsubsection{Component guessing}

The main protocol correctly identifies the components with probability $1 - o(1)$ using $O\left(n^{3/2} \log^{1/2} n\right)$ bits of communication, in a single round, with a common source of randomness. The steps are:
\begin{enumerate}
    \item each computer/node $v$ sends specific data to the coordinator based on its degree, which is also sent:
    \begin{itemize}
        \item if $\deg(v) \leq \sqrt{\frac{n}{\log n}}$, then it sends $N(v)$ to the coordinator;
        \item otherwise, it samples $k \leq \left\lfloor \sqrt{\frac{n}{\log n}} \right\rfloor$ neighbours \uar{}, and sends them to the leader.
    \end{itemize}
    \item the coordinator receives the degree-neighbourhood couples and reconstructs the partial graph $H$ from them. Observe that each component of $H$ is a subcomponent of some component of the graph $G$, and a complete component of $H$ is a component containing only nodes whose degree in $H$ is equal to the degree declared by themselves;
    \item the coordinator runs the cut-checking protocol for $\left\lceil 2 \sqrt{n \log n} \right\rceil$ times simultaneously on all the subsets of incomplete components\footnote{Note that there exist up to $2^{O(n)}$ such subsets, so this step can require exponential time. This lack in efficiency may be addressed by using the Gaussian elimination algorithm.};
    \item the coordinator claims that the minimal sets that were deemed correct by all the runs of the cut-checking protocol are the components.
\end{enumerate}

\begin{lemma}
    The protocol sends at most $O(\sqrt{n^3 \log n})$ bits through the network.
\end{lemma}

\begin{proof}
    Step 1 sends at most:
    \[
        n \left( \log n + \left\lfloor \sqrt{\frac{n}{\log n}} \right\rfloor \right) \in O \left( n \sqrt{\frac{n}{\log n}} \right)
    \]
    Step 3 performs multiple iterations of the cut checking protocol:
    \[
        \left\lceil 2 \sqrt{n \log n} \right\rceil \cdot n \in O(n \sqrt{n \log n})
    \]
    The other steps do not send any bit. In total:
    \[
        O \left( n \sqrt{\frac{n}{\log n}} \right) + O(n \sqrt{n \log n}) = O(\sqrt{n^3}) \cdot O \left( \oneover{\sqrt{\log n}} + \sqrt{\log n} \right) = O(\sqrt{n^3 \log n})
    \]
\end{proof}

\begin{theorem}
    The protocol outputs the correct components with probability $1 - o(1)$; 
\end{theorem}

\begin{proof}
    Since each incomplete component has at least one node with degree at least $\sqrt{\frac{n}{\log n}}$, each incomplete component must contain more than $\sqrt{\frac{n}{\log n}}$ nodes; therefore there are at most $\frac{n}{\sqrt {\frac{n}{\log n}}} = \sqrt{n \log n}$ incomplete components. The number of subsets of incomplete components is then at most $2^{\sqrt{n \log n}}$; the probability that any given set, that has one or more edges in its cuts, passes all the runs of the Cut-Checking algorithm is at most $2^{-2\sqrt{n \log n}}$. Therefore, by the union bound, the probability that some set that strictly includes a connected component passes all the tests is at most:
    \[
        2^{\sqrt{n \log n}} \cdot 2^{-2\sqrt{n \log n}} = 2^{-\sqrt{n \log n}} = o(1) \qedhere
    \]
\end{proof}