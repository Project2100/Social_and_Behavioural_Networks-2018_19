% !TEX spellcheck = en-US

\section{More distances}
	Let $A \vartriangle B = (A-B) \cup (B-A)$.

	Dice similarity:
	\begin{equation}
	\displaystyle D(A, B) = \frac{|A\cap B|}{|A\cap B| + \frac{1}{2}|A\vartriangle B|}
	\end{equation}
	
	Anderberg similarity:
	\begin{equation}
	\displaystyle An(A, B) = \frac{|A\cap B|}{|A\cap B| + 2|A\vartriangle B|}
	\end{equation}
	
	Generalizing Jaccard, Dice and Anderberg: 
	\begin{equation}
	\displaystyle S_\gamma(A, B) = \frac{|A\cap B|}{|A\cap B| + \gamma|A\vartriangle B|}
	\end{equation}
	By this third definition we can obtain $\jaccsim$ with $\gamma=1$, $D$ with $\gamma=\frac{1}{2}$, and $An$ with $\gamma=2$.
	
	\textit{Question}: for which values of $\gamma$ does an LSH exist for $S$? The next lemma helps us finding an answer.
	
	\lem (by Charikar) If a similarity $S$ admits an LSH, then the function $1-S$ must satisfy the triangular inequality, i.e. $\forall A,B,C \in U$ \\ $d(A,B) \leq d(A,C) + d(B,C)$, where the distance $d(A,B) = 1-S(A,B)$ is our function $1-S$.
	
	\newpage
	Proof: Let $E_{XY}$ be the event "$h(X) \neq h(Y)$"; since $S$ admits a LSH, we have:
	\begin{align*}
		d(A,B) &= 1-S(A,B) = \Pr_h(E_{AB}) = p_1 + p_2 + p_3 + p_4 \\
		d(A,C) &= 1-S(A,C) = \Pr_h(E_{AC}) = p_1 + p_3 + p_5 + p_7 \\
		d(B,C) &= 1-S(B,C) = \Pr_h(E_{BC}) = p_1 + p_2 + p_5 + p_6
	\end{align*}
	with the probabilities $p_i$ defined as in the following table, where an $X$ under the "may exist" column means that the corresponding probability is $0$ (it happens because transitivity generates contradictions, see example [\ref{ex:transitivity}]):

	\begin{tabular}{lllll}
		& $E_{AB}$ & $E_{AC}$ & $E_{BC}$ & may exist             \\
		$p_1$ & T         & T         & T         & $\checkmark$ \\
		$p_2$ & T         & T         & F         & $\checkmark$ \\
		$p_3$ & T         & F         & T         & $\checkmark$ \\
		$p_4$ & T         & F         & F         & X            \\
		$p_5$ & F         & T         & T         & $\checkmark$ \\
		$p_6$ & F         & T         & F         & X            \\
		$p_7$ & F         & F         & T         & X            \\
		$p_8$ & F         & F         & F         & $\checkmark$
	\end{tabular}

	\noindent Hence we have
	\begin{equation*}
		d(A,C) + d(B,C) = 2p_1 + p_2 + p_3 + 2p_5 \geq p_1 + p_2 + p_3 = d(A,B)
	\end{equation*}
	and so $1-S$ doesn't comply with the triangular inequality, QED.
	
	\obs Similarities are actually defined in most cases as the inverses of measures, which in turn give (oh so surprisingly!) a notion of distance.
	
	\cor By Charikar's lemma, we can prove that Dice's similarity cannot admit a LSH scheme.
	
	\textit{Proof} by counterexample: Assume $A=\{1\}, B=\{2\}, C=\{1, 2\}$, then use the triangular inequality over the distances: \\
	$D(A,C)=\frac{2}{3},\ D(B,C)=\frac{2}{3},\ D(A,B)=0$
	\begin{multline*}
		d(A,B) = 1- D(A,B) = 1 > \\
		\frac{2}{3} = (1-D(A,C)) + (1-D(B,C)) = d(A,C) + d(B,C)
	\end{multline*}
	hence it doesn't comply with the triangular inequality, QED.\\
	(Note that $D$ stands for Dice similarity and $d$ for distance)
	
	\obs Parameterizing this counterexample with $S_\gamma$, we obtain a bounds for $\gamma$: let $A=\{1\}, B=\{2\}, C=\{1, 2\}$ and \\
	$S_\gamma(A,C)=\frac{1}{1+\gamma},\ S_\gamma(B,C)=\frac{1}{1+\gamma},\ S_\gamma(A,B)=0$, thus
	\begin{multline*}
		1=1-S_\gamma(A,B)=d(A,B) > d(A,C) + d(B,C) = \\
		(1-S_\gamma(A,C)) + (1-S_\gamma(B,C)) = 2 \left( 1- \frac{1}{1+\gamma} \right) = \frac{2\gamma}{1 + \gamma}
	\end{multline*}
	from which we obtain
	$1 > \frac{2\gamma}{1 + \gamma} \Rightarrow \gamma < 1$. \\
	Hence, if $\gamma < 1$, the triangular inequality doesn't hold, so no LSH can exist, as in the case of the Dice similarity.
	
\section{Probability generating functions}
	
	\textit{Intuition}: A probability generating function (PGF) is a power series representation of a given probability distribution
	
	\textbf{Definition}: Given a (discrete) RV $X$, its \textbf{PGF} is the function:
	\begin{equation}
		\mathcal{G}en_X(\alpha)= \sum_{x=0}^{\infty}\Pr(X=x)\alpha^x
	\end{equation}
	(note that all outcomes appear by their probability).
	
	How to get back to pmf: $\displaystyle \Pr(X=x) = \frac{\mathcal{D}^x(\mathcal{G}en_X(0))}{x!}$  % TODO chi è D? e pmf?
	
	In other terms, a PGF $f$ is a function:
	\begin{equation}
		f(x)= \sum_{x=0}^{\infty}\left( p_i x^i \right)
	\end{equation}
	s.t. $p_i\geq 0 \forall i$ and $\sum_{x=0}^{\infty} p_i = 1$.
	
	\thm: If a similarity $S$ admits a LSH and a given function $f$ is a PGF, then $f(S)$ admits a LSH.
	\begin{equation*}
		f(S(A,B))=(f(S)) (A,B) =: T(A,B)
	\end{equation*}
	
	% TODO check
	% Afterthought: Are we "applying" a probability distribution over a similarity (which in turn, since it is lshable, means it has a probdist over a subset of hashfunctions)?
	
	Before going into the proof of the theorem we show a consequence of it.
	
	\obs Applying the theorem to the Jaccard similarity: \\
	Our function is $f_\gamma$, with $\gamma > 1$, defined as
	\begin{equation} \label{eq:pgf_jacc}
		\displaystyle f_\gamma(x) = \frac{x}{x+\gamma(1-x)}
	\end{equation} \\
	In order to proof $f_\gamma$ is a PGF, we have to demonstrate that the coefficients represent a probability distribution: i.e., they are all positive and they sum to zero. \\
	By applying the Taylor series expansion to [\ref{eq:pgf_jacc}], we get \\
	$\displaystyle
		f_\gamma(x) = \sum_{x=1}^{\infty}\left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1}x^i\right)
	$; now $f_\gamma$ is a power series, so all the coefficients are positive. \\
	In order for the sum of the coefficients to be equal to 1, the numerator must be equal to the denominator: \\
	$\displaystyle \sum_{i=1}^{\infty} \left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1} \right) = 1 \Rightarrow \sum_{i=1}^{\infty} \left( \left(1-\frac{1}{\gamma}\right)^i \right) = \gamma -1$. \\
	Indeed we have:
	\begin{flalign*}
		\sum_{i=1}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i
		&= \left(1 - \frac{1}{\gamma}\right)\sum_{i=0}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i &&\\
		&=^{\left(*\right)} \left(1 - \frac{1}{\gamma}\right) \frac{1}{1 - \left(1 - \frac{1}{\gamma}\right)} &&\\
		&= \frac{\gamma -1 }{\gamma}\frac{1}{1/\gamma} = \gamma -1 &&
	\end{flalign*}
	where the step marked with $\left(*\right)$ is due to the equality $\sum_{i=0}^{\infty} \alpha^i = \frac{1}{1-\alpha}$. \\
	And with this we proved that $f_\gamma$ is a PGF.
	
	Now we can apply PGF to a similarity $S_\gamma$:
	\begin{align*}
	f_\gamma (S_\gamma(A, B))
	&= f_\gamma \left( \frac{\abs{A \cap B}}{\abs{A \cup B}} \right) \\
	&= \frac{\frac{\abs{A \cap B}}{\abs{A \cup B}}}
		{\frac{\abs{A \cap B}}{\abs{A \cup B}} +
			\gamma \frac{\abs{A \cup B} -
				\abs{A \cap B}}{\abs{A \cup B}}} \\
	&= \frac{\abs{A \cap B}}
		{\abs{A \cap B} + \gamma \abs{A \vartriangle B}}
	 = S_\gamma(A, B)
	\end{align*}
	which in turn is LSH-able
	
\section{More about PGF}  % TODO la lasciamo questa sezione? ex 181008
	
	%% So it actually matches the previous wikipedian def
	\textit{Recap}: Given a universe $U$, a function $S \in U^2 \to [0, 1]$ is said to be a \textbf{LSHable similarity} iff exists a prob. distr. over (a family/subset of) the hash functions in $U$, such that: 
	\begin{equation}
	\forall \{X, Y\} \in \binom{U}{2}\ \ \Pr_h(h(X)=h(Y)) = S(X, Y)
	\end{equation}
	
	\thm \label{t:pgf_1} If a similarity $S$ is LSH-able and $f$ is a PGF, then $f(S)$ is LSHable.
	
	\textit{Equivalent statement}:
	\begin{equation}
	f(S) := T \in U^2 \to [0, 1] \text{ s.t. } \forall \{A, B\} \in \binom{U}{2}\ \ T(A, B) = f(S(A, B))
	\end{equation}
	
	% TODO check
	%Afterthought: could we just demonstrate that probability distributions are composable? lshability could be just a carried-over property...
	
	\lem \label{l:pgf_1} (L1) The similarity $O \in U^2 \to [0, 1]$ s.t. $O(A, B) \mapsto 1 \ \forall \{A, B\} \in \binom{U}{2}$ admits a LSH.
	
	\textit{Proof}: Give probability 1 to the constant hash function $h$: $h(A)=1 \ \forall A\in U$\\
	$\Pr(h(A)=h(B))=1=O(A,B) \ \forall \{A, B\} \in \binom{U}{2}$.
	
	% TODO mi sembra più chiaro così ^
	% Give prob. 1 to a constant function (duh!): $h \in U \to \mathbbm{1} A \mapsto 0 \forall A in U$
	
	\textit{Purpose}: This will be the base case for theorem proof...
	
	\lem \label{l:pgf_2} (L2) If $S$ and $T$ similarities over $U$ have a scheme, then $S \cdot T : (S \cdot T)(A, B) = S(A, B)\cdot T(A, B)$ has a scheme. \\
	I.e. LSHability is preserved upon composition/multiplication.
	
	\textit{Proof} by construction (Algorithm): 
	\begin{itemize}
		\item Sample hash functions for $S \cdot T$ as follows:
		\begin{enumerate}
			\item first, sample $h_S$ from the LSH for S;
			\item then, sample $h_T$ independently from the LSH for T;
		\end{enumerate}
		\item Return the hash function $h$ s.t.
		$H(A) = (h_S(A), h_T(A)) \ \forall A \in U$;
		\item Thus, $\forall \{A, B\} \in \binom{U}{2}$, we have:
		\begin{flalign*}
			\Pr_h(h(A)=h(B))
			&= \Pr_{h_S}(h_S(A)=h_S(B)) \cdot \Pr_{h_T}(h_T(A)=h_T(B)) &&\\
			&= S(A, B) \cdot T(A, B) &&
		\end{flalign*}
		by independency.
	\end{itemize}
	
	\lem \label{l:pgf_3} (L3) if $S$ is LSHable, then $S^i$ is LSHable, $\forall i \in \mathbb{N}$.
	
	\
	
	\textit{Proof} by induction:
	\begin{itemize}
		\item Base: $i=0$ and $S^0=O$ \\
			True for L1 [\ref{l:pgf_1}];
		\item Inductive hypothesis: $S^i$ is LSHable;
		\item Inductive step: $S^{i+1}$ is LSHable \\
			True because $S^{i+1}= S \cdot S^i$ is LSHable by L2 [\ref{l:pgf_2}], since $S$ has a scheme by def and $S^i$ has a scheme by inductive hypothesis.
	\end{itemize}
	
	\lem \label{l:pgf_4} (4) If $p_0, p_1, ..., p_i, ...$ is such that $\sum_{i=0}^{\infty}p_i=1$, $p_i\geq 0 \ \forall i$, and $S_0, S_1, ... , S_i, ...$ are lshable similarities, then $\sum_{i=0}^{\infty}p_iS_i$ is lshable.
	
	\textit{Proof} by scheme:
	\begin{enumerate}
	\item First, sample $i*$ at random from $\mathbb{N}$ with probability $p_0, ..., p_i, ...$;
	\item Then, sample $h$ from the hash functions (LSH) of $S_{i*}$ (that is, we are choosing which LSH to use);
	\item Thus we have:
	\begin{flalign*}
		\Pr_h(h(A)=h(B))
		&=\sum_{i=0}^{\infty}(p_i S_i(A, B)) &&\\
		&=\sum_{i=0}^{\infty}(\underbrace{\Pr(i=i*)}_{p_i} \cdot \underbrace{\Pr_h(h(A)=h(B) | i=i*))}_{S_i(A,B)} &&\\
	\end{flalign*}
	\end{enumerate}

	\obs This lemma is useful if we need a weighted average \\
	$W(A,B) = \sum_{i=0}^{\infty}(p_i S_i(A, B))$.
	
	\textit{Proof} of the theorem [\ref{t:pgf_1}]:
	\begin{itemize}
	\item We want to prove that $\sum_{i=0}^{\infty}p_iS^i$ has a scheme (is LSHable);
	\item By L3 [\ref{l:pgf_3}] we know $S^i$ has a scheme;
	\item By L4 [\ref{l:pgf_4}] we know the sum is lshable;
	\item Proven.
	\end{itemize}
	
	\obs This theorem tells us how to build a LSH: by concatenating the results of different hash functions, keeping the output small. \\ I.e. PGF is an approach for making schemes for similarities from other schemes.
	\ex $\sum_{i=1}^{a}(2^{-i} \cdot x^i)$.
	
	\section{A mention of the sketches}
	
	Like LSH, they're a means for simplify the storage of data; they are slower than LSH but allow you to keep interesting information in a small space.
	
	\textbf{Definition}: a sketch is a representation of big objects with small images. They can be used to retrieve interesting information about original objects without regain the whole original objects (i.e. they are not compression algorithms).
	
	\ex It's possible to go back to $\abs{A \cap B}$ from only $|A|, |B|, h(A), h(B),$ $\jaccsim(A,B)\pm\varepsilon$. Note that $\jaccsim(A,B)\pm\varepsilon$ can be obtained from $h(A), h(B)$ by applying the Chernoff Bounf [\ref{chernoff}] and the other data need only few bits to be stored.
	
	\ex It's possible to approximate $\frac{\abs{A \cap B}}{\abs{A \cap B} + \frac{1}{2}\abs{A \vartriangle B}}$ with \\
	 $\frac{1}{2} \cdot \abs{A \cup B} + \frac{1}{2} \cdot \abs{A \cap B}$.
	
	% TODO check
	% --------------------------------------------------------------
	% ??sorensen dice??cosine similarity?? inner product??johnson-lindenstrauss??
	
	%a sketch is n instance of a pgf which can be used to implement other similarities NONONONONOO
	
	
	
\section{Approximation of non-LSHable functions}

	What can we do if there is no LSH for the similarity we want/need to use? We can approximate a function without a LSH with an LSHable one.
	
	\ex \label{ex:appr_1} We can approximate $S_\gamma$ with $\jaccsim$ with an error of $\frac{1}{\gamma}$.
	
	% TODO check
	\thm (???) Let $f$ be a PGF, $\alpha \in [0, 1]$. \\
	\begin{flalign*}
		& \alpha f = \alpha f(S)\ |\ (1 - \alpha)T &&\\
		& T \in U^2 \to [0, 1] : \forall {t, t'} \in \binom{U}{2} \ T(t, t') = 0 \text{ but } T(t, t) = 1 &&\\
		& h(t)=-t &&
	\end{flalign*}
	
	For the $1$ case we wanted a banal partition, now with $0$ we want a punctual partition, so we need a hash function that assigns a distinct value to each argument. (You can actually use the identity) \\
	Not a good scheme, because we're not shrinking data.
	
	% TODO ???
	% GOTO %&
	
	\textbf{Definition}: Let $S: U^2 \to [0,1]$ be a similarity, then its \textbf{distortion} is the minimum* $\delta \geq 1$ s.t. $\exists$ an LSHable similarity $S'$ s.t. $\forall {A, B} \in \binom{U}{2}$ \\ $\frac{1}{\delta} \cdot S(A, B) \leq S'(A, B) \leq S(A, B)$, where minimum* is the lower bound (if $U$ isn't finite, there is no actual minimum).
	
	\obs $S'$ is the LSHable similarity closest to $S$; the more $\delta$ is near to $1$, the closer $S'$ is to $S$ (i.e. $\delta$ is the \textit{approximation factor}); if $\delta$ tends to $1$, then $S$ is LSHable.
	
	\ex \label{ex:appr_2} In the example \ref{ex:appr_1}, where we used $\jaccsim$ to approximate $S_\gamma$, we had $\delta \to \frac{1}{\gamma}$.
	
	\textit{Question}: How can I know if there is a better approximation than the one I found?
	
	\lem \label{l:center} (Center lemma) Let $S$ be a LSHable similarity s.t. $\exists \mathcal{X} \subseteq U : \forall \{x, x'\} \in \binom{\mathcal{X}}{2} \ S(x, x')=0$, then $\forall y \in U \ avg_{x \in \mathcal{X}}(S(X, Y)) \leq \frac{1}{\abs{\mathcal{X}}}$.
	
	\obs $\mathcal{X}$ is a set of the most different objects in $U$ and $y$ is a center in $U$; so the meaning of the lemma is that the average of the distances between each point and the center is $\leq \frac{1}{\abs{\mathcal{X}}}$.
	
	\obs $\mathcal{X}$ is actually a (possibly incomplete) section of the partition of $U$ induced by $h$.
	
	\obs (important) From the lemma, it trivially follows that \\ $\exists \ x^* \in \mathcal{X} : S(x^*, Y \leq \frac{1}{\abs{\mathcal{X}}})$.
	
	\
	
	\textit{Proof} of the Center lemma:
	\begin{itemize}
	\item Fix $y \in U$;
	\item If the hash function $h$ has positive probability in the (chosen) LSH for $S$, then $\forall {x, x'} \in \binom{\mathcal{X}}{2} \ h(x)\neq h(x')$ (otherwise we would have $S(x, x')>0$);
	\item Thus, $\forall$ hash functions with positive probability, there can exist at most one $x \in \mathcal{X}$ s.t. $h(x)=h(y)$ (by transitivity of equality);
	\item Than we have:
		\begin{flalign*}
			\sum_{x \in \mathcal{X}}S(x, y)
			&= \sum_{x \in \mathcal{X}}\Pr_h(h(x)=h(y)) &&\\
			&= \sum_{x \in \mathcal{X}}\sum_{h}\Pr(h\ \text{is chosen}) \cdot
				\underbrace{[h(x)=h(y)]}_{\text{pr. that $x=y$ with the choosen $h$}} &&\\
			&= \sum_h \Pr(h \text{ is chosen}) \cdot
				\sum_{x \in \mathcal{X}}[h(x)=h(y)] &&\\
			&\leq \sum_h \Pr(h \text{ is chosen}) = 1 &&
		\end{flalign*}
	(Note that he square brackets here are a boolean evaluation operator);
	\item Thus, $\sum_{x \in \mathcal{X}}S(x, y) \leq 1$, that implies that $avg(S(X, Y)) \leq \frac{1}{\abs{\mathcal{X}}}$, and the lemma is proven.
	\end{itemize}
		
	\ex We will now prove what we said about $S_\gamma$'s approximation in examples [\ref{ex:appr_1}] and [\ref{ex:appr_2}], using the lemma we have just demonstrated [\ref{l:center}]:
	\begin{itemize}
	\item Let us give some definitions:
		\begin{itemize}
			\item $S := S_\gamma \text{, with } 0 < \gamma < 1$,
			\item $U := 2^{[n]}=\{S|S\subseteq [n]\}$,  %insieme delle parti?
			\item $\mathcal{X} := \mathcal{P}_1([n])=\{ \{1\}, \{2\}, ..., \{n\} \}$;
		\end{itemize}
	\item So, by definition of $S_\gamma$, we have $S_\gamma(\{i\},\{j\}) = 0 \ \forall i \neq j \in [n]$, since $\{i\} \cap \{j\} = \emptyset$;
	\item Let us assume that $T$ (that will be our $S'$) finitely distorts $S_\gamma$, and $T$ is LSHable;
	\item Then $T(\{i\},\{j\}) = 0 \ \forall \{i, j\} in \binom{[n]}{2}$, since $S' \leq S_\gamma = 0$;
	\item Since $T$ is LSHable we can apply the center lemma:
	 	\begin{flalign*}
	 		& \exists \{i\} \in \mathcal{X} \text{ s.t. } T(\underbrace{\{i\}}_\text{our $x^*$}, \underbrace{[n]}_\text{our $y$}) \leq \frac{1}{\abs{\mathcal{X}}} = \frac{1}{n} &&\\
	 		& S_\gamma(\{i\}, [n]) = \frac{1}{1 + \gamma \cdot (n-1)} = \frac{1}{\gamma \cdot n + (1-\gamma)} &&\\
	 		& \text{so } \exists i \in [n] \text{ s.t. } S_\gamma(\{i\}, [n]) \geq \frac{1}{\gamma \cdot n + (1-\gamma)} &&\\
	 		& \text{but } T(\{i\}, [n]) \leq \frac{1}{n} &&\\
	 		& \text{thus } \frac{1}{\delta} \frac{1}{\gamma \cdot n + (1-\gamma)} = \frac{1}{\delta} S_\gamma(\{i\}, [n]) \leq T(\{i\}, [n]) \leq \frac{1}{n} &&\\
	 		& \text{hence } \frac{1}{\gamma + \frac{1-\gamma}{n}} = \frac{n}{\gamma \cdot n + (1-\gamma)} \leq \delta &&\\
	 		& \liminf_{n \to \infty} \delta \geq \frac{1}{\gamma} \text{ QED} &&
	 	\end{flalign*}
	 	i.e. the more $n$ grows, the more $\delta$ approaches $\frac{1}{\gamma}$;
	\item Now we know that the distortion of $S_\gamma$ is $\frac{1}{\gamma}$.
	\end{itemize}
	
	\ex Now we would like to apply the same method to the \textbf{cosine similarity} (a.k.a. \textit{inner product similarity}).
	\begin{itemize}
		\item Let's start again with some definitions:
		\begin{itemize}
			\item $U := \{ \vec{x} \ | \ \vec{x} \in \mathbb{R}_+^n, \ \abs{\abs{\vec{x}}}_2=1 \}$ \ \
			(i.e., $U$ is a positive hypersphere with the center in the origin and the radius equal to 1),
			\item $C \in U^2 \to [0,1]$ s.t. $C(\vec{x}, \vec{y}) = \left\langle \vec{x}, \vec{y} \right\rangle = \sum_{i=1}^n x_iy_i$;
		\end{itemize}
		\item Now, we want to know if $C$ has an LSH and, if not, what is its distortion;
		\item Another definition: \\
			$\mathcal{X}=\{ \vec{x}_1, \vec{x}_2, ..., \vec{x}_n \}$
			where $x_i(j)=\begin{cases}
				1 & \text{if}\ i=j\\
				0 & \text{otherwise}\\
			\end{cases} $ \\
			i.e. $\vec{x}_i=(0,...,0,1,0,...,0)$ with the only 0 in position $i$;
		\item So we have that $\abs{\abs{\vec{x}}}_2=\sqrt{1^2+0^2(n-1)}=1$;
		\item Moreover $C(\vec{x_i}, \vec{x_j})=0 \ \forall i\neq j$;
		\item Let $\vec{y}=\left( \underbrace{\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}}, ..., \frac{1}{\sqrt{n}}}_\text{n times} \right)$, then $\abs{\abs{\vec{y}}}_2 = \sqrt{\sum_{i=1}^n y_i^2} = \sqrt{\sum_{i=1}^n \frac{1}{n}}=1$;
		\item So, $C(\vec{x_i}, \vec{y})= \sum_{j=1}^n \left( x_i(j) \cdot y_i(j) \right) = x_i(i) \cdot y_i(i) = \frac{1}{\sqrt{n}}$;
		\item Then $\delta \geq \sqrt{n}$, so, unlike before, the distortion is big and grows bigger with $n$.
	\end{itemize}

	\ex \textbf{Weighed Jaccard} is a generalization of $\jaccsim$ for vectors: \\ $\mathcal{WJ}=\frac
		{\sum_i \min(x_i, y_i)}{\sum_i \max(x_i, y_i)}$.
	
	\ex \textbf{Sim Hash} is an LSH scheme similar to the cosine similarity:
	\begin{itemize}
		\item $\mathcal{CS}(\vec{x},\vec{y})=
			\cos(\theta_{\vec{x},\vec{y}}) $;
		\item $\mathcal{SH}(\vec{x},\vec{y})=
			1-\frac{\theta_{\vec{x},\vec{y}}}{\pi} $;
		\item $\mathcal{SH}$ is high if the angle $\theta$ is small;
		\item $\frac{\theta_{\vec{x},\vec{y}}}{\pi}$ is the probability that an hyperplane divides $\vec{x}$ and $\vec{y}$, where the hyperplane is a threshold between similar and dissimilar elements of the universe, so it creates a partition of the universe in two sets.
	\end{itemize}
	
	
	
	
	